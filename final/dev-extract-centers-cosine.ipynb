{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from VGG16_GAP import VGG16_GAP\n",
    "# from VGG16_flatten import VGG16_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as imageio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from progress.bar import Bar\n",
    "# from ipywidgets import IntProgress\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/cclin/ntu/dlcv2018/DLCV2018SPRING/final/label_dict.pkl', 'rb') as f:\n",
    "    y_dict = pickle.load(f)\n",
    "\n",
    "with open('/home/cclin/ntu/dlcv2018/DLCV2018SPRING/final/inv_label_dict.pkl', 'rb') as f:\n",
    "    inv_y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('save/inv_label_dict.pkl', 'rb') as f:\n",
    "#     inv_y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR = \"/data/put_data/cclin/ntu/dlcv2018/final/\"\n",
    "TRAIN_DIR = HOME_DIR+\"dlcv_final_2_dataset/train/\"\n",
    "VALID_DIR = HOME_DIR+\"dlcv_final_2_dataset/val/\"\n",
    "TEST_DIR = HOME_DIR+\"dlcv_final_2_dataset/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/train_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])\n",
    "dvalid = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/val_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = list(TRAIN_DIR+dtrain.img)\n",
    "valid_list = list(VALID_DIR+dvalid.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = list()\n",
    "for root, subdir, files in os.walk(TEST_DIR):\n",
    "    for file in sorted(files):\n",
    "        if '.jpg' in file:\n",
    "            test_list.append(os.path.join(TEST_DIR, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readImgList(file_list):\n",
    "    images = list()\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(i, end=\"\\r\")\n",
    "        img = imageio.imread(file)\n",
    "        img = img.astype(int)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformLabel(id_list, y_dict):\n",
    "    label = list()\n",
    "    for uid in list(id_list):\n",
    "        label.append(y_dict[uid])\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(class_numbers, num_classes):\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (56475, 218, 178, 3)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = readImgList(train_list)\n",
    "print(\"train:\", Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: (7211, 218, 178, 3)\n"
     ]
    }
   ],
   "source": [
    "Xvalid = readImgList(valid_list)\n",
    "print(\"valid:\", Xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: (7152, 218, 178, 3)\n"
     ]
    }
   ],
   "source": [
    "Xtest = readImgList(test_list)\n",
    "print(\"valid:\", Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain = transformLabel(list(dtrain.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yvalid = transformLabel(list(dvalid.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytrain = one_hot_encoding(ytrain, len(y_dict))\n",
    "Yvalid = one_hot_encoding(yvalid, len(y_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scope_name = \"Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VGG16_GAP(scope_name=scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAG_save_dir = \"/data/put_data/cmchang/DLCV_final/CL_v3-cont2_dynamic_gap_L5_v3_rescale0-1_save_linear/\"\n",
    "FLAG_init_from = FLAG_save_dir + \"para_dict.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model started\n",
      "npy file loaded\n",
      "build model finished: 0s\n"
     ]
    }
   ],
   "source": [
    "model.build(vgg16_npy_path=FLAG_init_from,\n",
    "            shape=Xtrain.shape[1:],\n",
    "            classes=len(y_dict),\n",
    "            conv_pre_training=True,\n",
    "            fc_pre_training=True,\n",
    "            new_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP under test: [1.]\n",
      "[None, 218, 178, 3]\n",
      "[None, 218, 178, 32]\n",
      "AFTER [None, 218, 178, 32]\n",
      "[None, 109, 89, 47]\n",
      "AFTER [None, 109, 89, 47]\n",
      "[None, 109, 89, 63]\n",
      "AFTER [None, 109, 89, 63]\n",
      "[None, 55, 45, 98]\n",
      "AFTER [None, 55, 45, 98]\n",
      "[None, 55, 45, 134]\n",
      "AFTER [None, 55, 45, 134]\n",
      "[None, 55, 45, 147]\n",
      "AFTER [None, 55, 45, 147]\n",
      "[None, 28, 23, 153]\n",
      "AFTER [None, 28, 23, 153]\n",
      "[None, 28, 23, 209]\n",
      "AFTER [None, 28, 23, 209]\n",
      "[None, 28, 23, 181]\n",
      "AFTER [None, 28, 23, 181]\n",
      "[None, 14, 12, 177]\n",
      "AFTER [None, 14, 12, 177]\n",
      "[None, 14, 12, 180]\n",
      "AFTER [None, 14, 12, 180]\n",
      "[None, 14, 12, 230]\n",
      "AFTER [None, 14, 12, 230]\n",
      "Set dp operations finished: 0s\n"
     ]
    }
   ],
   "source": [
    "dp = [1.0]\n",
    "model.set_idp_operation(dp=dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_number_params(para_dict):\n",
    "#     n = 0\n",
    "#     for k,v in sorted(para_dict.items()):\n",
    "#         if 'bn_mean' in k:\n",
    "#             continue\n",
    "#         elif 'bn_variance' in k:\n",
    "#             continue\n",
    "#         elif 'gamma' in k:\n",
    "#             continue\n",
    "#         elif 'beta' in k:\n",
    "#             continue\n",
    "#         elif 'conv' in k or 'fc' in k:\n",
    "#             n += get_params_shape(v[0].shape.as_list())\n",
    "#             n += get_params_shape(v[1].shape.as_list())\n",
    "#     return n\n",
    "\n",
    "# def get_params_shape(shape):\n",
    "#     n = 1\n",
    "#     for dim in shape:\n",
    "#         n = n*dim\n",
    "#     return n\n",
    "\n",
    "# def count_flops(para_dict, net_shape):\n",
    "#     input_shape = (3 ,32 ,32) # Format:(channels, rows,cols)\n",
    "#     total_flops_per_layer = 0\n",
    "#     input_count = 0\n",
    "#     for k,v in sorted(para_dict.items()):\n",
    "#         if 'bn_mean' in k:\n",
    "#             continue\n",
    "#         elif 'bn_variance' in k:\n",
    "#             continue\n",
    "#         elif 'gamma' in k:\n",
    "#             continue\n",
    "#         elif 'beta' in k:\n",
    "#             continue\n",
    "#         elif 'fc' in k:\n",
    "#             continue\n",
    "#         elif 'conv' in k:\n",
    "#             conv_filter = v[0].shape.as_list()[3::-1] # (64 ,3 ,3 ,3)  # Format: (num_filters, channels, rows, cols)\n",
    "#             stride = 1\n",
    "#             padding = 1\n",
    "\n",
    "#             if conv_filter[1] == 0:\n",
    "#                 n = conv_filter[2] * conv_filter[3] # vector_length\n",
    "#             else:\n",
    "#                 n = conv_filter[1] * conv_filter[2] * conv_filter[3]  # vector_length\n",
    "\n",
    "#             flops_per_instance = n + ( n -1)    # general defination for number of flops (n: multiplications and n-1: additions)\n",
    "\n",
    "#             num_instances_per_filter = (( input_shape[1] - conv_filter[2] + 2 * padding) / stride) + 1  # for rows\n",
    "#             num_instances_per_filter *= ((input_shape[1] - conv_filter[2] + 2 * padding) / stride) + 1  # multiplying with cols\n",
    "\n",
    "#             flops_per_filter = num_instances_per_filter * flops_per_instance\n",
    "#             total_flops_per_layer += flops_per_filter * conv_filter[0]  # multiply with number of filters\n",
    "\n",
    "#             total_flops_per_layer += conv_filter[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "#             input_shape = net_shape[input_count].as_list()[3:0:-1]\n",
    "#             input_count +=1\n",
    "\n",
    "#     total_flops_per_layer += net_shape[-1].as_list()[1] *2360*2\n",
    "#     return total_flops_per_layer\n",
    "\n",
    "# def countFlopsParas(net):\n",
    "#     total_flops = count_flops(net.para_dict, net.net_shape)\n",
    "#     if total_flops / 1e9 > 1:   # for Giga Flops\n",
    "#         print(total_flops/ 1e9 ,'{}'.format('GFlops'))\n",
    "#     else:\n",
    "#         print(total_flops / 1e6 ,'{}'.format('MFlops'))\n",
    "\n",
    "#     total_params = count_number_params(net.para_dict)\n",
    "\n",
    "#     if total_params / 1e9 > 1:   # for Giga Flops\n",
    "#         print(total_params/ 1e9 ,'{}'.format('G'))\n",
    "#     else:\n",
    "#         print(total_params / 1e6 ,'{}'.format('M'))\n",
    "    \n",
    "#     return total_flops, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_number_params(para_dict):\n",
    "    n = 0\n",
    "    for k,v in sorted(para_dict.items()):\n",
    "        if 'bn_mean' in k:\n",
    "            continue\n",
    "        elif 'bn_variance' in k:\n",
    "            continue\n",
    "        elif 'gamma' in k:\n",
    "            continue\n",
    "        elif 'beta' in k:\n",
    "            continue\n",
    "        elif 'conv' in k or 'fc' in k:\n",
    "            n += get_params_shape(v[0].shape.as_list())\n",
    "            n += get_params_shape(v[1].shape.as_list())\n",
    "    return n\n",
    "\n",
    "def get_params_shape(shape):\n",
    "    n = 1\n",
    "    for dim in shape:\n",
    "        n = n*dim\n",
    "    return n\n",
    "\n",
    "def count_flops(para_dict, net_shape, input_shape=(3, 218, 178)):\n",
    "    # Format:(channels, rows,cols)\n",
    "    total_flops_per_layer = 0\n",
    "    input_count = 0\n",
    "    for k,v in sorted(para_dict.items()):\n",
    "        if 'bn_mean' in k:\n",
    "            continue\n",
    "        elif 'bn_variance' in k:\n",
    "            continue\n",
    "        elif 'gamma' in k:\n",
    "            continue\n",
    "        elif 'beta' in k:\n",
    "            continue\n",
    "        elif 'fc' in k:\n",
    "            continue\n",
    "        elif 'conv' in k:\n",
    "            conv_filter = v[0].shape.as_list()[3::-1] # (64 ,3 ,3 ,3)  # Format: (num_filters, channels, rows, cols)\n",
    "            stride = 1\n",
    "            padding = 1\n",
    "\n",
    "            if conv_filter[1] == 0:\n",
    "                n = conv_filter[2] * conv_filter[3] # vector_length\n",
    "            else:\n",
    "                n = conv_filter[1] * conv_filter[2] * conv_filter[3]  # vector_length\n",
    "\n",
    "            flops_per_instance = n + ( n -1)    # general defination for number of flops (n: multiplications and n-1: additions)\n",
    "\n",
    "            num_instances_per_filter = (( input_shape[1] - conv_filter[2] + 2 * padding) / stride) + 1  # for rows\n",
    "            num_instances_per_filter *= ((input_shape[2] - conv_filter[3] + 2 * padding) / stride) + 1  # multiplying with cols\n",
    "\n",
    "            flops_per_filter = num_instances_per_filter * flops_per_instance\n",
    "            total_flops_per_layer += flops_per_filter * conv_filter[0]  # multiply with number of filters\n",
    "\n",
    "            total_flops_per_layer += conv_filter[0] * input_shape[1] * input_shape[2] # bias\n",
    "\n",
    "            input_shape = net_shape[input_count].as_list()[3:0:-1]\n",
    "            input_count +=1\n",
    "\n",
    "    total_flops_per_layer += net_shape[-1].as_list()[1] *2360*2\n",
    "    return total_flops_per_layer\n",
    "\n",
    "def countFlopsParas(net):\n",
    "    total_flops = count_flops(net.para_dict, net.net_shape)\n",
    "    if total_flops / 1e9 > 1:   # for Giga Flops\n",
    "        print(total_flops/ 1e9 ,'{}'.format('GFlops'))\n",
    "    else:\n",
    "        print(total_flops / 1e6 ,'{}'.format('MFlops'))\n",
    "\n",
    "    total_params = count_number_params(net.para_dict)\n",
    "\n",
    "    if total_params / 1e9 > 1:   # for Giga Flops\n",
    "        print(total_params/ 1e9 ,'{}'.format('G'))\n",
    "    else:\n",
    "        print(total_params / 1e6 ,'{}'.format('M'))\n",
    "    \n",
    "    return total_flops, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.875544996 GFlops\n",
      "4.048755 M\n",
      "Flops: 6875.544996 M, Paras: 4.048755 M\n"
     ]
    }
   ],
   "source": [
    "flops, params = countFlopsParas(model)\n",
    "FLAG_flops_M = flops/1e6\n",
    "FLAG_params_M = params/1e6\n",
    "print(\"Flops: %3f M, Paras: %3f M\" % (flops/1e6, params/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "282\n",
      "36\n",
      "35\r"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables())\n",
    "    print(\"Initialized\")\n",
    "    output_train = []\n",
    "    output_valid = []\n",
    "    output_test = []\n",
    "    for i in range(int(Xtrain.shape[0]/200+1)):\n",
    "        print(i, end=\"\\r\")\n",
    "        st = i*200\n",
    "        ed = min((i+1)*200, Xtrain.shape[0])\n",
    "        prob = sess.run(model.features, feed_dict={model.x: Xtrain[st:ed,:], \n",
    "                                                    model.is_train: False})\n",
    "        output_train.append(prob)\n",
    "    print()\n",
    "    \n",
    "    for i in range(int(Xvalid.shape[0]/200+1)):\n",
    "        print(i, end=\"\\r\")\n",
    "        st = i*200\n",
    "        ed = min((i+1)*200, Xvalid.shape[0])\n",
    "        prob = sess.run(model.features, feed_dict={model.x: Xvalid[st:ed,:], \n",
    "                                                    model.is_train: False})\n",
    "        output_valid.append(prob)\n",
    "    print()\n",
    "    \n",
    "    for i in range(int(Xtest.shape[0]/200+1)):\n",
    "        print(i, end=\"\\r\")\n",
    "        st = i*200\n",
    "        ed = min((i+1)*200, Xtest.shape[0])\n",
    "        prob = sess.run(model.features, feed_dict={model.x: Xtest[st:ed,:], \n",
    "                                                    model.is_train: False})\n",
    "        output_test.append(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute center and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'data', 'put_data', 'cmchang', 'DLCV_final', 'CL_v3-cont2_dynamic_gap_L5_v3_rescale0-1_save_linear', '']\n",
      "CL_v3-cont2_dynamic_gap_L5_v3_rescale0-1_save_linear\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "\n",
    "print(re.split(pattern='\\/', string=FLAG_save_dir))\n",
    "basic_model_name = re.split(pattern='\\/', string=FLAG_save_dir)[-2]\n",
    "print(basic_model_name)\n",
    "\n",
    "saved_path = os.path.join('/data/put_data/cclin/ntu/dlcv2018/final/results', basic_model_name)\n",
    "if not os.path.exists(saved_path):\n",
    "    os.makedirs(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56475, 423)\n"
     ]
    }
   ],
   "source": [
    "EX_train = np.concatenate(output_train,)\n",
    "print(EX_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7211, 423)\n"
     ]
    }
   ],
   "source": [
    "EX_valid = np.concatenate(output_valid,)\n",
    "print(EX_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7152, 423)\n"
     ]
    }
   ],
   "source": [
    "EX_test = np.concatenate(output_test,)\n",
    "print(EX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(saved_path, 'EX_train.npy'), EX_train)\n",
    "np.save(os.path.join(saved_path, 'EX_valid.npy'), EX_valid)\n",
    "np.save(os.path.join(saved_path, 'EX_test.npy'), EX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = np.zeros((len(y_dict), EX_train.shape[1]))\n",
    "for i in range(len(y_dict)):\n",
    "    centers[i,:] = np.mean(EX_train[ytrain==i,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360, 423)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(arr=centers,file=FLAG_save_dir+\"centers.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7211, 2360)\n",
      "(7211,)\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(EX_valid, centers)\n",
    "print(cos_sim.shape)\n",
    "pred_valid = np.argmax(cos_sim, axis=1)\n",
    "print(pred_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7867147413673554"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(yvalid), list(pred_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63686, 423)\n"
     ]
    }
   ],
   "source": [
    "EX_all = np.concatenate((EX_train, EX_valid))\n",
    "print(EX_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63686,)\n"
     ]
    }
   ],
   "source": [
    "Y = np.concatenate([ytrain, yvalid])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers_all = np.zeros((len(y_dict), EX_all.shape[1]))\n",
    "for i in range(len(y_dict)):\n",
    "    centers_all[i,:] = np.mean(EX_all[Y==i,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360, 423)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7152, 2360)\n",
      "(7152,)\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(EX_test, centers_all)\n",
    "print(cos_sim.shape)\n",
    "pred_test = np.argmax(cos_sim, axis=1)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_id = list()\n",
    "for pred in pred_test:\n",
    "    final_id.append(inv_y_dict[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doutput = pd.DataFrame({'id':np.arange(len(final_id))+1, 'ans':final_id}, columns=['id','ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'data', 'put_data', 'cmchang', 'DLCV_final', 'CL_v3-cont2_dynamic_gap_L5_v3_rescale0-1_save_linear', '']\n",
      "CL_v3-cont2_dynamic_gap_L5_v3_rescale0-1_save_linear\n",
      "cos_pred_val78_CL_v3-cont2_dynamic_gap_L5_v3_rescale0-1_save_linear.csv\n"
     ]
    }
   ],
   "source": [
    "output_name = \"cos_pred_val78_\" + basic_model_name + \".csv\"\n",
    "print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doutput.to_csv(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backup: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "# neigh.fit(centers, np.arange(len(y_dict)))\n",
    "# pred_valid = neigh.predict(EX_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### compute error for each class (use validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class = []\n",
    "for i in range(len(y_dict)):\n",
    "    accuracy_per_class.append(accuracy_score(list(yvalid[yvalid==i]), list(pred_valid[yvalid==i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 14,\n",
       " 1: 15,\n",
       " 2: 17,\n",
       " 3: 21,\n",
       " 4: 22,\n",
       " 5: 23,\n",
       " 6: 25,\n",
       " 7: 31,\n",
       " 8: 32,\n",
       " 9: 38,\n",
       " 10: 47,\n",
       " 11: 56,\n",
       " 12: 59,\n",
       " 13: 65,\n",
       " 14: 66,\n",
       " 15: 69,\n",
       " 16: 78,\n",
       " 17: 82,\n",
       " 18: 85,\n",
       " 19: 94,\n",
       " 20: 95,\n",
       " 21: 114,\n",
       " 22: 122,\n",
       " 23: 142,\n",
       " 24: 147,\n",
       " 25: 160,\n",
       " 26: 171,\n",
       " 27: 192,\n",
       " 28: 199,\n",
       " 29: 209,\n",
       " 30: 220,\n",
       " 31: 234,\n",
       " 32: 235,\n",
       " 33: 248,\n",
       " 34: 273,\n",
       " 35: 284,\n",
       " 36: 289,\n",
       " 37: 291,\n",
       " 38: 295,\n",
       " 39: 301,\n",
       " 40: 303,\n",
       " 41: 305,\n",
       " 42: 313,\n",
       " 43: 318,\n",
       " 44: 334,\n",
       " 45: 343,\n",
       " 46: 348,\n",
       " 47: 349,\n",
       " 48: 353,\n",
       " 49: 363,\n",
       " 50: 365,\n",
       " 51: 371,\n",
       " 52: 394,\n",
       " 53: 395,\n",
       " 54: 397,\n",
       " 55: 403,\n",
       " 56: 404,\n",
       " 57: 409,\n",
       " 58: 417,\n",
       " 59: 433,\n",
       " 60: 447,\n",
       " 61: 449,\n",
       " 62: 467,\n",
       " 63: 468,\n",
       " 64: 481,\n",
       " 65: 487,\n",
       " 66: 492,\n",
       " 67: 494,\n",
       " 68: 495,\n",
       " 69: 496,\n",
       " 70: 499,\n",
       " 71: 510,\n",
       " 72: 516,\n",
       " 73: 534,\n",
       " 74: 539,\n",
       " 75: 545,\n",
       " 76: 553,\n",
       " 77: 562,\n",
       " 78: 565,\n",
       " 79: 567,\n",
       " 80: 600,\n",
       " 81: 614,\n",
       " 82: 632,\n",
       " 83: 649,\n",
       " 84: 650,\n",
       " 85: 659,\n",
       " 86: 661,\n",
       " 87: 665,\n",
       " 88: 676,\n",
       " 89: 679,\n",
       " 90: 719,\n",
       " 91: 735,\n",
       " 92: 742,\n",
       " 93: 800,\n",
       " 94: 801,\n",
       " 95: 844,\n",
       " 96: 861,\n",
       " 97: 871,\n",
       " 98: 873,\n",
       " 99: 894,\n",
       " 100: 918,\n",
       " 101: 924,\n",
       " 102: 951,\n",
       " 103: 953,\n",
       " 104: 958,\n",
       " 105: 971,\n",
       " 106: 973,\n",
       " 107: 975,\n",
       " 108: 977,\n",
       " 109: 978,\n",
       " 110: 981,\n",
       " 111: 985,\n",
       " 112: 986,\n",
       " 113: 987,\n",
       " 114: 988,\n",
       " 115: 991,\n",
       " 116: 992,\n",
       " 117: 993,\n",
       " 118: 997,\n",
       " 119: 1003,\n",
       " 120: 1008,\n",
       " 121: 1011,\n",
       " 122: 1014,\n",
       " 123: 1018,\n",
       " 124: 1023,\n",
       " 125: 1025,\n",
       " 126: 1027,\n",
       " 127: 1031,\n",
       " 128: 1032,\n",
       " 129: 1033,\n",
       " 130: 1034,\n",
       " 131: 1035,\n",
       " 132: 1037,\n",
       " 133: 1043,\n",
       " 134: 1044,\n",
       " 135: 1050,\n",
       " 136: 1051,\n",
       " 137: 1054,\n",
       " 138: 1059,\n",
       " 139: 1061,\n",
       " 140: 1062,\n",
       " 141: 1066,\n",
       " 142: 1088,\n",
       " 143: 1089,\n",
       " 144: 1090,\n",
       " 145: 1092,\n",
       " 146: 1103,\n",
       " 147: 1113,\n",
       " 148: 1120,\n",
       " 149: 1128,\n",
       " 150: 1135,\n",
       " 151: 1136,\n",
       " 152: 1137,\n",
       " 153: 1140,\n",
       " 154: 1149,\n",
       " 155: 1151,\n",
       " 156: 1153,\n",
       " 157: 1154,\n",
       " 158: 1164,\n",
       " 159: 1170,\n",
       " 160: 1175,\n",
       " 161: 1180,\n",
       " 162: 1181,\n",
       " 163: 1187,\n",
       " 164: 1188,\n",
       " 165: 1194,\n",
       " 166: 1198,\n",
       " 167: 1201,\n",
       " 168: 1204,\n",
       " 169: 1209,\n",
       " 170: 1210,\n",
       " 171: 1213,\n",
       " 172: 1218,\n",
       " 173: 1221,\n",
       " 174: 1223,\n",
       " 175: 1224,\n",
       " 176: 1228,\n",
       " 177: 1231,\n",
       " 178: 1245,\n",
       " 179: 1246,\n",
       " 180: 1250,\n",
       " 181: 1258,\n",
       " 182: 1261,\n",
       " 183: 1264,\n",
       " 184: 1270,\n",
       " 185: 1272,\n",
       " 186: 1275,\n",
       " 187: 1279,\n",
       " 188: 1281,\n",
       " 189: 1287,\n",
       " 190: 1288,\n",
       " 191: 1289,\n",
       " 192: 1291,\n",
       " 193: 1293,\n",
       " 194: 1297,\n",
       " 195: 1301,\n",
       " 196: 1302,\n",
       " 197: 1303,\n",
       " 198: 1305,\n",
       " 199: 1306,\n",
       " 200: 1314,\n",
       " 201: 1318,\n",
       " 202: 1321,\n",
       " 203: 1322,\n",
       " 204: 1324,\n",
       " 205: 1326,\n",
       " 206: 1330,\n",
       " 207: 1335,\n",
       " 208: 1336,\n",
       " 209: 1337,\n",
       " 210: 1339,\n",
       " 211: 1341,\n",
       " 212: 1346,\n",
       " 213: 1348,\n",
       " 214: 1351,\n",
       " 215: 1356,\n",
       " 216: 1359,\n",
       " 217: 1361,\n",
       " 218: 1362,\n",
       " 219: 1368,\n",
       " 220: 1370,\n",
       " 221: 1371,\n",
       " 222: 1373,\n",
       " 223: 1377,\n",
       " 224: 1382,\n",
       " 225: 1383,\n",
       " 226: 1384,\n",
       " 227: 1389,\n",
       " 228: 1392,\n",
       " 229: 1393,\n",
       " 230: 1394,\n",
       " 231: 1395,\n",
       " 232: 1410,\n",
       " 233: 1411,\n",
       " 234: 1415,\n",
       " 235: 1420,\n",
       " 236: 1422,\n",
       " 237: 1423,\n",
       " 238: 1435,\n",
       " 239: 1439,\n",
       " 240: 1440,\n",
       " 241: 1443,\n",
       " 242: 1446,\n",
       " 243: 1447,\n",
       " 244: 1448,\n",
       " 245: 1454,\n",
       " 246: 1455,\n",
       " 247: 1467,\n",
       " 248: 1468,\n",
       " 249: 1469,\n",
       " 250: 1470,\n",
       " 251: 1471,\n",
       " 252: 1472,\n",
       " 253: 1474,\n",
       " 254: 1481,\n",
       " 255: 1488,\n",
       " 256: 1489,\n",
       " 257: 1490,\n",
       " 258: 1493,\n",
       " 259: 1496,\n",
       " 260: 1498,\n",
       " 261: 1499,\n",
       " 262: 1508,\n",
       " 263: 1513,\n",
       " 264: 1517,\n",
       " 265: 1519,\n",
       " 266: 1524,\n",
       " 267: 1531,\n",
       " 268: 1535,\n",
       " 269: 1537,\n",
       " 270: 1538,\n",
       " 271: 1542,\n",
       " 272: 1544,\n",
       " 273: 1549,\n",
       " 274: 1550,\n",
       " 275: 1559,\n",
       " 276: 1560,\n",
       " 277: 1565,\n",
       " 278: 1566,\n",
       " 279: 1568,\n",
       " 280: 1569,\n",
       " 281: 1574,\n",
       " 282: 1578,\n",
       " 283: 1580,\n",
       " 284: 1581,\n",
       " 285: 1582,\n",
       " 286: 1585,\n",
       " 287: 1587,\n",
       " 288: 1588,\n",
       " 289: 1589,\n",
       " 290: 1590,\n",
       " 291: 1591,\n",
       " 292: 1601,\n",
       " 293: 1603,\n",
       " 294: 1604,\n",
       " 295: 1607,\n",
       " 296: 1611,\n",
       " 297: 1614,\n",
       " 298: 1624,\n",
       " 299: 1625,\n",
       " 300: 1628,\n",
       " 301: 1629,\n",
       " 302: 1631,\n",
       " 303: 1633,\n",
       " 304: 1634,\n",
       " 305: 1639,\n",
       " 306: 1641,\n",
       " 307: 1642,\n",
       " 308: 1646,\n",
       " 309: 1648,\n",
       " 310: 1649,\n",
       " 311: 1651,\n",
       " 312: 1657,\n",
       " 313: 1660,\n",
       " 314: 1664,\n",
       " 315: 1672,\n",
       " 316: 1673,\n",
       " 317: 1675,\n",
       " 318: 1686,\n",
       " 319: 1689,\n",
       " 320: 1691,\n",
       " 321: 1696,\n",
       " 322: 1698,\n",
       " 323: 1700,\n",
       " 324: 1708,\n",
       " 325: 1711,\n",
       " 326: 1713,\n",
       " 327: 1714,\n",
       " 328: 1715,\n",
       " 329: 1722,\n",
       " 330: 1723,\n",
       " 331: 1725,\n",
       " 332: 1726,\n",
       " 333: 1731,\n",
       " 334: 1743,\n",
       " 335: 1744,\n",
       " 336: 1749,\n",
       " 337: 1755,\n",
       " 338: 1757,\n",
       " 339: 1761,\n",
       " 340: 1770,\n",
       " 341: 1784,\n",
       " 342: 1793,\n",
       " 343: 1805,\n",
       " 344: 1807,\n",
       " 345: 1809,\n",
       " 346: 1810,\n",
       " 347: 1811,\n",
       " 348: 1812,\n",
       " 349: 1815,\n",
       " 350: 1817,\n",
       " 351: 1839,\n",
       " 352: 1840,\n",
       " 353: 1842,\n",
       " 354: 1844,\n",
       " 355: 1852,\n",
       " 356: 1854,\n",
       " 357: 1862,\n",
       " 358: 1871,\n",
       " 359: 1873,\n",
       " 360: 1874,\n",
       " 361: 1875,\n",
       " 362: 1877,\n",
       " 363: 1882,\n",
       " 364: 1884,\n",
       " 365: 1885,\n",
       " 366: 1886,\n",
       " 367: 1888,\n",
       " 368: 1895,\n",
       " 369: 1899,\n",
       " 370: 1903,\n",
       " 371: 1908,\n",
       " 372: 1912,\n",
       " 373: 1914,\n",
       " 374: 1916,\n",
       " 375: 1927,\n",
       " 376: 1929,\n",
       " 377: 1934,\n",
       " 378: 1936,\n",
       " 379: 1938,\n",
       " 380: 1944,\n",
       " 381: 1946,\n",
       " 382: 1947,\n",
       " 383: 1951,\n",
       " 384: 1956,\n",
       " 385: 1960,\n",
       " 386: 1962,\n",
       " 387: 1967,\n",
       " 388: 1977,\n",
       " 389: 1984,\n",
       " 390: 1985,\n",
       " 391: 1986,\n",
       " 392: 1990,\n",
       " 393: 1991,\n",
       " 394: 1993,\n",
       " 395: 1994,\n",
       " 396: 1996,\n",
       " 397: 1997,\n",
       " 398: 1998,\n",
       " 399: 1999,\n",
       " 400: 2002,\n",
       " 401: 2006,\n",
       " 402: 2011,\n",
       " 403: 2014,\n",
       " 404: 2015,\n",
       " 405: 2017,\n",
       " 406: 2019,\n",
       " 407: 2020,\n",
       " 408: 2026,\n",
       " 409: 2030,\n",
       " 410: 2031,\n",
       " 411: 2034,\n",
       " 412: 2044,\n",
       " 413: 2046,\n",
       " 414: 2054,\n",
       " 415: 2055,\n",
       " 416: 2061,\n",
       " 417: 2063,\n",
       " 418: 2065,\n",
       " 419: 2066,\n",
       " 420: 2067,\n",
       " 421: 2070,\n",
       " 422: 2072,\n",
       " 423: 2073,\n",
       " 424: 2078,\n",
       " 425: 2079,\n",
       " 426: 2083,\n",
       " 427: 2084,\n",
       " 428: 2086,\n",
       " 429: 2087,\n",
       " 430: 2095,\n",
       " 431: 2097,\n",
       " 432: 2101,\n",
       " 433: 2110,\n",
       " 434: 2111,\n",
       " 435: 2114,\n",
       " 436: 2117,\n",
       " 437: 2119,\n",
       " 438: 2120,\n",
       " 439: 2122,\n",
       " 440: 2124,\n",
       " 441: 2125,\n",
       " 442: 2130,\n",
       " 443: 2134,\n",
       " 444: 2136,\n",
       " 445: 2141,\n",
       " 446: 2150,\n",
       " 447: 2153,\n",
       " 448: 2155,\n",
       " 449: 2156,\n",
       " 450: 2158,\n",
       " 451: 2160,\n",
       " 452: 2167,\n",
       " 453: 2168,\n",
       " 454: 2173,\n",
       " 455: 2174,\n",
       " 456: 2180,\n",
       " 457: 2182,\n",
       " 458: 2190,\n",
       " 459: 2193,\n",
       " 460: 2194,\n",
       " 461: 2195,\n",
       " 462: 2201,\n",
       " 463: 2202,\n",
       " 464: 2219,\n",
       " 465: 2221,\n",
       " 466: 2225,\n",
       " 467: 2231,\n",
       " 468: 2235,\n",
       " 469: 2236,\n",
       " 470: 2238,\n",
       " 471: 2239,\n",
       " 472: 2240,\n",
       " 473: 2242,\n",
       " 474: 2243,\n",
       " 475: 2245,\n",
       " 476: 2246,\n",
       " 477: 2249,\n",
       " 478: 2250,\n",
       " 479: 2253,\n",
       " 480: 2259,\n",
       " 481: 2261,\n",
       " 482: 2263,\n",
       " 483: 2270,\n",
       " 484: 2279,\n",
       " 485: 2288,\n",
       " 486: 2295,\n",
       " 487: 2296,\n",
       " 488: 2301,\n",
       " 489: 2303,\n",
       " 490: 2307,\n",
       " 491: 2309,\n",
       " 492: 2311,\n",
       " 493: 2314,\n",
       " 494: 2316,\n",
       " 495: 2318,\n",
       " 496: 2325,\n",
       " 497: 2331,\n",
       " 498: 2332,\n",
       " 499: 2334,\n",
       " 500: 2335,\n",
       " 501: 2337,\n",
       " 502: 2348,\n",
       " 503: 2349,\n",
       " 504: 2353,\n",
       " 505: 2373,\n",
       " 506: 2374,\n",
       " 507: 2393,\n",
       " 508: 2398,\n",
       " 509: 2406,\n",
       " 510: 2411,\n",
       " 511: 2416,\n",
       " 512: 2419,\n",
       " 513: 2425,\n",
       " 514: 2429,\n",
       " 515: 2433,\n",
       " 516: 2443,\n",
       " 517: 2445,\n",
       " 518: 2451,\n",
       " 519: 2461,\n",
       " 520: 2463,\n",
       " 521: 2464,\n",
       " 522: 2467,\n",
       " 523: 2471,\n",
       " 524: 2472,\n",
       " 525: 2483,\n",
       " 526: 2484,\n",
       " 527: 2496,\n",
       " 528: 2507,\n",
       " 529: 2509,\n",
       " 530: 2516,\n",
       " 531: 2518,\n",
       " 532: 2520,\n",
       " 533: 2522,\n",
       " 534: 2523,\n",
       " 535: 2524,\n",
       " 536: 2529,\n",
       " 537: 2533,\n",
       " 538: 2534,\n",
       " 539: 2537,\n",
       " 540: 2546,\n",
       " 541: 2547,\n",
       " 542: 2550,\n",
       " 543: 2562,\n",
       " 544: 2570,\n",
       " 545: 2572,\n",
       " 546: 2573,\n",
       " 547: 2574,\n",
       " 548: 2579,\n",
       " 549: 2580,\n",
       " 550: 2588,\n",
       " 551: 2599,\n",
       " 552: 2602,\n",
       " 553: 2603,\n",
       " 554: 2610,\n",
       " 555: 2611,\n",
       " 556: 2612,\n",
       " 557: 2613,\n",
       " 558: 2643,\n",
       " 559: 2758,\n",
       " 560: 2761,\n",
       " 561: 2762,\n",
       " 562: 2772,\n",
       " 563: 2779,\n",
       " 564: 2790,\n",
       " 565: 2791,\n",
       " 566: 2792,\n",
       " 567: 2793,\n",
       " 568: 2805,\n",
       " 569: 2808,\n",
       " 570: 2810,\n",
       " 571: 2813,\n",
       " 572: 2819,\n",
       " 573: 2820,\n",
       " 574: 2823,\n",
       " 575: 2831,\n",
       " 576: 2837,\n",
       " 577: 2843,\n",
       " 578: 2844,\n",
       " 579: 2846,\n",
       " 580: 2847,\n",
       " 581: 2859,\n",
       " 582: 2860,\n",
       " 583: 2861,\n",
       " 584: 2863,\n",
       " 585: 2867,\n",
       " 586: 2870,\n",
       " 587: 2879,\n",
       " 588: 2880,\n",
       " 589: 2882,\n",
       " 590: 2883,\n",
       " 591: 2884,\n",
       " 592: 2887,\n",
       " 593: 2892,\n",
       " 594: 2897,\n",
       " 595: 2898,\n",
       " 596: 2899,\n",
       " 597: 2908,\n",
       " 598: 2909,\n",
       " 599: 2910,\n",
       " 600: 2914,\n",
       " 601: 2921,\n",
       " 602: 2924,\n",
       " 603: 2926,\n",
       " 604: 2930,\n",
       " 605: 2937,\n",
       " 606: 2938,\n",
       " 607: 2945,\n",
       " 608: 2952,\n",
       " 609: 2955,\n",
       " 610: 2957,\n",
       " 611: 2960,\n",
       " 612: 2965,\n",
       " 613: 2969,\n",
       " 614: 2972,\n",
       " 615: 2973,\n",
       " 616: 2981,\n",
       " 617: 2983,\n",
       " 618: 2984,\n",
       " 619: 2987,\n",
       " 620: 2996,\n",
       " 621: 3010,\n",
       " 622: 3014,\n",
       " 623: 3017,\n",
       " 624: 3020,\n",
       " 625: 3022,\n",
       " 626: 3024,\n",
       " 627: 3027,\n",
       " 628: 3028,\n",
       " 629: 3029,\n",
       " 630: 3033,\n",
       " 631: 3034,\n",
       " 632: 3035,\n",
       " 633: 3037,\n",
       " 634: 3040,\n",
       " 635: 3043,\n",
       " 636: 3047,\n",
       " 637: 3051,\n",
       " 638: 3052,\n",
       " 639: 3055,\n",
       " 640: 3056,\n",
       " 641: 3059,\n",
       " 642: 3067,\n",
       " 643: 3080,\n",
       " 644: 3081,\n",
       " 645: 3084,\n",
       " 646: 3090,\n",
       " 647: 3098,\n",
       " 648: 3111,\n",
       " 649: 3113,\n",
       " 650: 3117,\n",
       " 651: 3118,\n",
       " 652: 3119,\n",
       " 653: 3127,\n",
       " 654: 3129,\n",
       " 655: 3130,\n",
       " 656: 3134,\n",
       " 657: 3135,\n",
       " 658: 3140,\n",
       " 659: 3145,\n",
       " 660: 3148,\n",
       " 661: 3149,\n",
       " 662: 3157,\n",
       " 663: 3166,\n",
       " 664: 3167,\n",
       " 665: 3168,\n",
       " 666: 3175,\n",
       " 667: 3181,\n",
       " 668: 3182,\n",
       " 669: 3184,\n",
       " 670: 3185,\n",
       " 671: 3186,\n",
       " 672: 3188,\n",
       " 673: 3195,\n",
       " 674: 3198,\n",
       " 675: 3203,\n",
       " 676: 3205,\n",
       " 677: 3207,\n",
       " 678: 3211,\n",
       " 679: 3212,\n",
       " 680: 3214,\n",
       " 681: 3227,\n",
       " 682: 3231,\n",
       " 683: 3238,\n",
       " 684: 3248,\n",
       " 685: 3255,\n",
       " 686: 3263,\n",
       " 687: 3270,\n",
       " 688: 3272,\n",
       " 689: 3278,\n",
       " 690: 3279,\n",
       " 691: 3280,\n",
       " 692: 3294,\n",
       " 693: 3304,\n",
       " 694: 3305,\n",
       " 695: 3312,\n",
       " 696: 3315,\n",
       " 697: 3316,\n",
       " 698: 3317,\n",
       " 699: 3324,\n",
       " 700: 3325,\n",
       " 701: 3328,\n",
       " 702: 3329,\n",
       " 703: 3330,\n",
       " 704: 3332,\n",
       " 705: 3336,\n",
       " 706: 3338,\n",
       " 707: 3356,\n",
       " 708: 3357,\n",
       " 709: 3364,\n",
       " 710: 3376,\n",
       " 711: 3377,\n",
       " 712: 3383,\n",
       " 713: 3386,\n",
       " 714: 3389,\n",
       " 715: 3397,\n",
       " 716: 3398,\n",
       " 717: 3400,\n",
       " 718: 3404,\n",
       " 719: 3407,\n",
       " 720: 3410,\n",
       " 721: 3413,\n",
       " 722: 3421,\n",
       " 723: 3431,\n",
       " 724: 3432,\n",
       " 725: 3436,\n",
       " 726: 3438,\n",
       " 727: 3440,\n",
       " 728: 3449,\n",
       " 729: 3465,\n",
       " 730: 3469,\n",
       " 731: 3475,\n",
       " 732: 3477,\n",
       " 733: 3478,\n",
       " 734: 3480,\n",
       " 735: 3485,\n",
       " 736: 3490,\n",
       " 737: 3491,\n",
       " 738: 3496,\n",
       " 739: 3500,\n",
       " 740: 3501,\n",
       " 741: 3504,\n",
       " 742: 3508,\n",
       " 743: 3521,\n",
       " 744: 3522,\n",
       " 745: 3524,\n",
       " 746: 3542,\n",
       " 747: 3546,\n",
       " 748: 3556,\n",
       " 749: 3562,\n",
       " 750: 3569,\n",
       " 751: 3575,\n",
       " 752: 3587,\n",
       " 753: 3600,\n",
       " 754: 3614,\n",
       " 755: 3619,\n",
       " 756: 3620,\n",
       " 757: 3623,\n",
       " 758: 3625,\n",
       " 759: 3641,\n",
       " 760: 3643,\n",
       " 761: 3648,\n",
       " 762: 3661,\n",
       " 763: 3664,\n",
       " 764: 3670,\n",
       " 765: 3678,\n",
       " 766: 3681,\n",
       " 767: 3696,\n",
       " 768: 3697,\n",
       " 769: 3699,\n",
       " 770: 3701,\n",
       " 771: 3705,\n",
       " 772: 3706,\n",
       " 773: 3708,\n",
       " 774: 3714,\n",
       " 775: 3719,\n",
       " 776: 3725,\n",
       " 777: 3727,\n",
       " 778: 3730,\n",
       " 779: 3745,\n",
       " 780: 3751,\n",
       " 781: 3755,\n",
       " 782: 3760,\n",
       " 783: 3762,\n",
       " 784: 3768,\n",
       " 785: 3782,\n",
       " 786: 3788,\n",
       " 787: 3795,\n",
       " 788: 3808,\n",
       " 789: 3811,\n",
       " 790: 3817,\n",
       " 791: 3826,\n",
       " 792: 3835,\n",
       " 793: 3845,\n",
       " 794: 3846,\n",
       " 795: 3847,\n",
       " 796: 3853,\n",
       " 797: 3854,\n",
       " 798: 3857,\n",
       " 799: 3858,\n",
       " 800: 3859,\n",
       " 801: 3862,\n",
       " 802: 3864,\n",
       " 803: 3875,\n",
       " 804: 3880,\n",
       " 805: 3882,\n",
       " 806: 3886,\n",
       " 807: 3895,\n",
       " 808: 3896,\n",
       " 809: 3901,\n",
       " 810: 3911,\n",
       " 811: 3912,\n",
       " 812: 3915,\n",
       " 813: 3928,\n",
       " 814: 3929,\n",
       " 815: 3930,\n",
       " 816: 3941,\n",
       " 817: 3957,\n",
       " 818: 3959,\n",
       " 819: 3960,\n",
       " 820: 3962,\n",
       " 821: 3980,\n",
       " 822: 3981,\n",
       " 823: 3984,\n",
       " 824: 3986,\n",
       " 825: 3989,\n",
       " 826: 3993,\n",
       " 827: 3994,\n",
       " 828: 4014,\n",
       " 829: 4017,\n",
       " 830: 4027,\n",
       " 831: 4028,\n",
       " 832: 4030,\n",
       " 833: 4031,\n",
       " 834: 4035,\n",
       " 835: 4048,\n",
       " 836: 4049,\n",
       " 837: 4051,\n",
       " 838: 4054,\n",
       " 839: 4055,\n",
       " 840: 4061,\n",
       " 841: 4068,\n",
       " 842: 4070,\n",
       " 843: 4076,\n",
       " 844: 4077,\n",
       " 845: 4080,\n",
       " 846: 4087,\n",
       " 847: 4094,\n",
       " 848: 4099,\n",
       " 849: 4102,\n",
       " 850: 4110,\n",
       " 851: 4112,\n",
       " 852: 4114,\n",
       " 853: 4119,\n",
       " 854: 4125,\n",
       " 855: 4126,\n",
       " 856: 4135,\n",
       " 857: 4136,\n",
       " 858: 4137,\n",
       " 859: 4138,\n",
       " 860: 4139,\n",
       " 861: 4141,\n",
       " 862: 4143,\n",
       " 863: 4146,\n",
       " 864: 4147,\n",
       " 865: 4149,\n",
       " 866: 4150,\n",
       " 867: 4151,\n",
       " 868: 4153,\n",
       " 869: 4154,\n",
       " 870: 4157,\n",
       " 871: 4161,\n",
       " 872: 4162,\n",
       " 873: 4163,\n",
       " 874: 4164,\n",
       " 875: 4165,\n",
       " 876: 4166,\n",
       " 877: 4168,\n",
       " 878: 4172,\n",
       " 879: 4174,\n",
       " 880: 4177,\n",
       " 881: 4181,\n",
       " 882: 4183,\n",
       " 883: 4190,\n",
       " 884: 4197,\n",
       " 885: 4204,\n",
       " 886: 4209,\n",
       " 887: 4210,\n",
       " 888: 4213,\n",
       " 889: 4219,\n",
       " 890: 4220,\n",
       " 891: 4221,\n",
       " 892: 4225,\n",
       " 893: 4236,\n",
       " 894: 4238,\n",
       " 895: 4242,\n",
       " 896: 4244,\n",
       " 897: 4245,\n",
       " 898: 4247,\n",
       " 899: 4251,\n",
       " 900: 4252,\n",
       " 901: 4255,\n",
       " 902: 4256,\n",
       " 903: 4257,\n",
       " 904: 4258,\n",
       " 905: 4261,\n",
       " 906: 4262,\n",
       " 907: 4263,\n",
       " 908: 4265,\n",
       " 909: 4268,\n",
       " 910: 4276,\n",
       " 911: 4286,\n",
       " 912: 4288,\n",
       " 913: 4290,\n",
       " 914: 4292,\n",
       " 915: 4293,\n",
       " 916: 4302,\n",
       " 917: 4303,\n",
       " 918: 4304,\n",
       " 919: 4306,\n",
       " 920: 4308,\n",
       " 921: 4309,\n",
       " 922: 4310,\n",
       " 923: 4311,\n",
       " 924: 4312,\n",
       " 925: 4314,\n",
       " 926: 4315,\n",
       " 927: 4316,\n",
       " 928: 4321,\n",
       " 929: 4323,\n",
       " 930: 4326,\n",
       " 931: 4328,\n",
       " 932: 4334,\n",
       " 933: 4336,\n",
       " 934: 4338,\n",
       " 935: 4346,\n",
       " 936: 4352,\n",
       " 937: 4355,\n",
       " 938: 4357,\n",
       " 939: 4372,\n",
       " 940: 4373,\n",
       " 941: 4374,\n",
       " 942: 4388,\n",
       " 943: 4392,\n",
       " 944: 4393,\n",
       " 945: 4395,\n",
       " 946: 4407,\n",
       " 947: 4410,\n",
       " 948: 4414,\n",
       " 949: 4418,\n",
       " 950: 4419,\n",
       " 951: 4421,\n",
       " 952: 4423,\n",
       " 953: 4424,\n",
       " 954: 4431,\n",
       " 955: 4433,\n",
       " 956: 4435,\n",
       " 957: 4436,\n",
       " 958: 4437,\n",
       " 959: 4438,\n",
       " 960: 4439,\n",
       " 961: 4441,\n",
       " 962: 4446,\n",
       " 963: 4447,\n",
       " 964: 4448,\n",
       " 965: 4450,\n",
       " 966: 4453,\n",
       " 967: 4458,\n",
       " 968: 4460,\n",
       " 969: 4462,\n",
       " 970: 4465,\n",
       " 971: 4472,\n",
       " 972: 4473,\n",
       " 973: 4474,\n",
       " 974: 4478,\n",
       " 975: 4483,\n",
       " 976: 4487,\n",
       " 977: 4493,\n",
       " 978: 4496,\n",
       " 979: 4498,\n",
       " 980: 4511,\n",
       " 981: 4514,\n",
       " 982: 4516,\n",
       " 983: 4518,\n",
       " 984: 4521,\n",
       " 985: 4527,\n",
       " 986: 4532,\n",
       " 987: 4537,\n",
       " 988: 4539,\n",
       " 989: 4543,\n",
       " 990: 4547,\n",
       " 991: 4549,\n",
       " 992: 4551,\n",
       " 993: 4554,\n",
       " 994: 4555,\n",
       " 995: 4558,\n",
       " 996: 4560,\n",
       " 997: 4561,\n",
       " 998: 4562,\n",
       " 999: 4566,\n",
       " ...}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.min(ytrain))\n",
    "\n",
    "\n",
    "inv_y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " nan,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " nan,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5714285714285714,\n",
       " 0.0,\n",
       " 0.8333333333333334,\n",
       " 0.42857142857142855,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.8333333333333334,\n",
       " 0.7,\n",
       " 0.6666666666666666,\n",
       " 0.6,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.0,\n",
       " nan,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " nan,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.5,\n",
       " 0.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " nan,\n",
       " nan,\n",
       " 0.25,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.4,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " nan,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.8333333333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.8333333333333334,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " nan,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8333333333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.8333333333333334,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.4,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.4,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.8333333333333334,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 1.0,\n",
       " 0.4,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.8333333333333334,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.7142857142857143,\n",
       " nan,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.625,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " nan,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8333333333333334,\n",
       " 0.875,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8333333333333334,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.4,\n",
       " 0.8333333333333334,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.25,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.5,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.8333333333333334,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.8333333333333334,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.4,\n",
       " 1.0,\n",
       " 0.2,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.7142857142857143,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.4,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5,\n",
       " nan,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 0.8571428571428571,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.2857142857142857,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8333333333333334,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.5,\n",
       " 0.7142857142857143,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 1.0,\n",
       " nan,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.7142857142857143,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 0.8571428571428571,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.5,\n",
       " nan,\n",
       " 0.0,\n",
       " 0.6,\n",
       " nan,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.875,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.4,\n",
       " 0.8,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.42857142857142855,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.4,\n",
       " 0.6666666666666666,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " nan,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.25,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 1.0,\n",
       " nan,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.4,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.8333333333333334,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.8,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " nan,\n",
       " nan,\n",
       " 0.75,\n",
       " 0.5,\n",
       " nan,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6666666666666666,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_per_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
